# noahgrad

there's a lot of duplicate code and under-optimized components. ship fast, fix later. this is a learning exersize, after all.

**features**
- tensors
- feed-forward mechanisms
- autograd / backpropagation
- loss functions
- optimizers
- layer modules

**todo**
- abstract out functions into classes
- gpu support (via jax? mlx feels like cheating)
